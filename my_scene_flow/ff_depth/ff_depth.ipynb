{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算深度\n",
    "\n",
    "## 方法\n",
    "將FlowFormer後銜接一個NN，訓練NN透過左右影像的flow回歸出深度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import flowformer元件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building  model...\n"
     ]
    }
   ],
   "source": [
    "# import flowformer元件\n",
    "from ast import List, Tuple\n",
    "import sys\n",
    "\n",
    "from sympy import root\n",
    "sys.path.append('core')\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from configs.submission import get_cfg\n",
    "from core.utils.misc import process_cfg\n",
    "import core.datasets\n",
    "from core.utils import flow_viz\n",
    "from core.utils import frame_utils\n",
    "import cv2\n",
    "import math\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from core.FlowFormer import build_flowformer\n",
    "\n",
    "from core.utils.utils import InputPadder, forward_interpolate\n",
    "import itertools\n",
    "\n",
    "import flow_compute\n",
    "import torch.utils.data as data\n",
    "\n",
    "TRAIN_SIZE = [432, 960]\n",
    "plt.rcParams['font.sans-serif'] = ['DFKai-SB']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 讀取深度\n",
    "def depth_read(filename) -> torch.float64:\n",
    "    # loads depth map D from png file\n",
    "    # and returns it as a numpy array,\n",
    "    # for details see readme.txt\n",
    "\n",
    "    depth_png = np.array(Image.open(filename), dtype=int)\n",
    "    # make sure we have a proper 16bit depth map here.. not 8bit!\n",
    "    assert (np.max(depth_png) > 255)\n",
    "\n",
    "    depth = depth_png.astype(float) / 256.\n",
    "    depth[depth_png == 0] = -1.\n",
    "    return torch.from_numpy(depth)\n",
    "\n",
    "\n",
    "depth = depth_read(r\"E:\\datasets\\KITTI_Depth Prediction\\data_depth_annotated\\train\\2011_09_26_drive_0001_sync\\proj_depth\\groundtruth\\image_02\\0000000005.png\")\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ck\\.conda\\envs\\flowformer_torch20\\Lib\\pathlib.py:573\u001b[0m, in \u001b[0;36mPurePath._cparts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_cparts\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WindowsPath' object has no attribute '_cached_cparts'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples\n\u001b[1;32m---> 76\u001b[0m my_kitti \u001b[38;5;241m=\u001b[39m \u001b[43mKITTI_Depth_Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m my_kitti_val \u001b[38;5;241m=\u001b[39m KITTI_Depth_Dataset(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m, in \u001b[0;36mKITTI_Depth_Dataset.__init__\u001b[1;34m(self, KITTI_path, depth_path, flow_path, type)\u001b[0m\n\u001b[0;32m     25\u001b[0m date\u001b[38;5;241m=\u001b[39mdrive[:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(date, drive)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 取得RGB影像\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m path_02s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m((KITTI_path \u001b[38;5;241m/\u001b[39m date \u001b[38;5;241m/\u001b[39m drive \u001b[38;5;241m/\u001b[39m\n\u001b[0;32m     29\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_02\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m     30\u001b[0m path_03s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m     31\u001b[0m     (KITTI_path \u001b[38;5;241m/\u001b[39m date \u001b[38;5;241m/\u001b[39m drive \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_03\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 取得預計算flow\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ck\\.conda\\envs\\flowformer_torch20\\Lib\\pathlib.py:593\u001b[0m, in \u001b[0;36mPurePath.__lt__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, PurePath) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m other\u001b[38;5;241m.\u001b[39m_flavour:\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m--> 593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cparts\u001b[49m \u001b[38;5;241m<\u001b[39m other\u001b[38;5;241m.\u001b[39m_cparts\n",
      "File \u001b[1;32mc:\\Users\\ck\\.conda\\envs\\flowformer_torch20\\Lib\\pathlib.py:575\u001b[0m, in \u001b[0;36mPurePath._cparts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_cparts\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_cparts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flavour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcasefold_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_cparts\n",
      "File \u001b[1;32mc:\\Users\\ck\\.conda\\envs\\flowformer_torch20\\Lib\\pathlib.py:190\u001b[0m, in \u001b[0;36m_WindowsFlavour.casefold_parts\u001b[1;34m(self, parts)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcasefold_parts\u001b[39m(\u001b[38;5;28mself\u001b[39m, parts):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ck\\.conda\\envs\\flowformer_torch20\\Lib\\pathlib.py:190\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcasefold_parts\u001b[39m(\u001b[38;5;28mself\u001b[39m, parts):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [p\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parts]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定義及宣告深度Dataset\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
    "class KITTI_Depth_Dataset(data.Dataset):\n",
    "    def __init__(self, KITTI_path=r'E:\\datasets\\KITTI', depth_path=r'E:\\datasets\\KITTI_Depth Prediction\\data_depth_annotated', flow_path=r'E:\\ck\\master\\ff_depth\\flow_pre_compute', type=\"train\"):\n",
    "        self.depth_02_paths=[]\n",
    "        self.depth_03_paths = []\n",
    "        self.image_02_paths = []\n",
    "        self.image_03_paths = []\n",
    "        self.flows=[]\n",
    "        \n",
    "        KITTI_path = Path(KITTI_path)\n",
    "        depth_paths = Path(depth_path)\n",
    "        depth_paths /= type\n",
    "        depth_paths = depth_paths.iterdir()\n",
    "        flow_path=Path(flow_path)\n",
    "\n",
    "        for path in depth_paths:\n",
    "            # 取得深度影像\n",
    "            path_depth_02s = sorted((path / \"proj_depth\" / 'groundtruth' / \"image_02\").glob(\"*.png\"))\n",
    "            path_depth_03s = sorted((path / \"proj_depth\" / 'groundtruth' / \"image_03\").glob(\"*.png\"))\n",
    "\n",
    "            # 取得深度影像日期、車次\n",
    "            drive=path.name\n",
    "            date=drive[:10]\n",
    "            # print(date, drive)\n",
    "            # 取得RGB影像\n",
    "            path_02s = sorted((KITTI_path / date / drive /\n",
    "                              \"image_02\" / \"data\").glob(\"*.png\"))[5:-5]\n",
    "            path_03s = sorted(\n",
    "                (KITTI_path / date / drive / \"image_03\" / \"data\").glob(\"*.png\"))[5:-5]\n",
    "            \n",
    "            # 取得預計算flow\n",
    "            flow_paths=flow_path.joinpath(drive).glob(\"*.pt\")\n",
    "\n",
    "            # 檢查長度是否相同\n",
    "            if len(path_02s)!=len(path_03s)!=len(path_depth_02s)!=len(flow_paths):\n",
    "                print(\n",
    "                    f\"data at {drive} is not the same! {path_02s} vs {path_03s} vs {path_depth_02s} vs {flow_paths}\")\n",
    "                # print(path_depth_02s)\n",
    "                # print(path_02s)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # 將資料加入陣列\n",
    "            for path_depth_02,path_depth_03 in zip(path_depth_02s,path_depth_03s):\n",
    "                self.depth_02_paths.append(str(path_depth_02))\n",
    "                self.depth_03_paths.append(str(path_depth_03))\n",
    "            for path_02,path_03 in zip(path_02s,path_03s):\n",
    "                self.image_02_paths.append(str(path_02))\n",
    "                self.image_03_paths.append(str(path_03))\n",
    "            for path_flow in flow_paths:\n",
    "                self.flows.append(path_flow)\n",
    "\n",
    "        print(f\"Add {len(self.depth_02_paths)} depth_02 path\")\n",
    "        print(f\"Add {len(self.depth_03_paths)} depth_03 path\")\n",
    "        print(f\"Add {len(self.image_02_paths)} image_02 path\")\n",
    "        print(f\"Add {len(self.image_03_paths)} image_03 path\")\n",
    "        print(f\"Add {len(self.flows)} flow path\")\n",
    "        self.n_samples = len(self.depth_02_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if type(index)==int:\n",
    "            image1, image2 = flow_compute.prepare_image(self.image_02_paths[index], self.image_03_paths[index], keep_size=True)\n",
    "            depth = depth_read(self.depth_02_paths[index])\n",
    "            flow=torch.load(self.flows[index])\n",
    "            return image1, image2, depth, flow\n",
    "        elif type(index)==slice:\n",
    "            image_02s=list(item for item in self.image_02_paths[index])\n",
    "            image_03s=list(item for item in self.image_03_paths[index])\n",
    "            image_depths=list(item for item in self.depth_02_paths[index])\n",
    "            image_flows = list(item for item in self.flows[index])\n",
    "            ans_image_02s=[]\n",
    "            ans_image_03s=[]\n",
    "            ans_image_depths=[]\n",
    "            ans_image_flows=[]\n",
    "            for image_02,image_03,image_depth,image_flow in zip(image_02s,image_03s,image_depths,image_flows):\n",
    "                image1, image2 = flow_compute.prepare_image(\n",
    "                    image_02, image_03, keep_size=True)\n",
    "                depth = depth_read(image_depth)\n",
    "                flow = torch.load(image_flow)\n",
    "                ans_image_02s.append(image1)\n",
    "                ans_image_03s.append(image2)\n",
    "                ans_image_depths.append(depth)\n",
    "                ans_image_flows.append(flow)\n",
    "            return ans_image_02s, ans_image_03s, ans_image_depths, ans_image_flows\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "my_kitti = KITTI_Depth_Dataset()\n",
    "my_kitti_val = KITTI_Depth_Dataset(type=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試計算光流(legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing image...\n",
      "E:\\datasets\\KITTI\\2011_09_26\\2011_09_26_drive_0001_sync\\image_02\\data\\0000000005.png, E:\\datasets\\KITTI\\2011_09_26\\2011_09_26_drive_0001_sync\\image_03\\data\\0000000005.png\n",
      "image1: torch.Size([3, 375, 1242]), E:\\datasets\\KITTI\\2011_09_26\\2011_09_26_drive_0001_sync\\image_02\\data\\0000000005.png\n",
      "image2: torch.Size([3, 375, 1242]), E:\\datasets\\KITTI\\2011_09_26\\2011_09_26_drive_0001_sync\\image_03\\data\\0000000005.png\n",
      "depth: torch.Size([375, 1242]), E:\\datasets\\KITTI_Depth Prediction\\data_depth_annotated\\train\\2011_09_26_drive_0001_sync\\proj_depth\\groundtruth\\image_02\\0000000005.png\n",
      "computing flow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ck\\.conda\\envs\\flowformer_torch20\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow: (375, 1242, 2)\n"
     ]
    }
   ],
   "source": [
    "path1, path2, depth_path = my_kitti[0]\n",
    "image1, image2 = flow_compute.prepare_image(path1, path2, keep_size=True)\n",
    "depth = depth_read(depth_path)\n",
    "print(f\"image1: {image1.shape}, {path1}\")\n",
    "print(f\"image2: {image2.shape}, {path2}\")\n",
    "print(f\"depth: {depth.shape}, {depth_path}\")\n",
    "with torch.no_grad():\n",
    "    flow = flow_compute.compute_flow(image1, image2)\n",
    "print(f\"flow: {flow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立NN網路\n",
    "\n",
    "參考:  \n",
    "[1]: https://ithelp.ithome.com.tw/m/articles/10289699 \"IT幫幫忙 Pytorch Feedforward Neural Network\"  \n",
    "[2]: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/pytorch_simple_fullynet.py#L68 \"ML/Pytorch/Basics/pytorch_simple_fullynet.py\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        # Our first linear layer take input_size, in this case 784 nodes to 50\n",
    "        # and our second linear layer takes 50 to the num_classes we have, in\n",
    "        # this case 10.\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "\n",
    "# Load Data\n",
    "train_loader = DataLoader(dataset=my_kitti,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=my_kitti_val,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練NN網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent or adam step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 儲存NN網路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入NN網路"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flowformer_torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
